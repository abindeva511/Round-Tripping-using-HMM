{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages and loading in the data set \n",
    "from utils_pos import get_word_tag, preprocess  \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the training corpus list\n",
      "['<\\thidden_0\\n', 'Kochi\\thidden_2\\n', 'Rishikesh\\thidden_1\\n', 'Hyderabad\\thidden_4\\n', 'Chennai\\thidden_3\\n']\n"
     ]
    }
   ],
   "source": [
    "# load in the training corpus\n",
    "with open(\"round_trip_routes.pos\", 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "\n",
    "print(f\"A few items of the training corpus list\")\n",
    "print(training_corpus[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few items of the vocabulary list\n",
      "['Delhi', 'Mumbai', 'Kolkata', 'Chennai', 'Bengaluru', 'Hyderabad', 'Ahmedabad', 'Jaipur', 'Agra', 'Varanasi', 'Amritsar', 'Kochi', 'Goa', 'Udaipur', 'Rishikesh', '<', '>', '']\n",
      "\n",
      "A few items at the end of the vocabulary list\n",
      "['Delhi', 'Mumbai', 'Kolkata', 'Chennai', 'Bengaluru', 'Hyderabad', 'Ahmedabad', 'Jaipur', 'Agra', 'Varanasi', 'Amritsar', 'Kochi', 'Goa', 'Udaipur', 'Rishikesh', '<', '>', '']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"city_names.txt\", 'r') as f:\n",
    "    voc_l = f.read().split('\\n')\n",
    "\n",
    "print(\"A few items of the vocabulary list\")\n",
    "print(voc_l[0:50])\n",
    "print()\n",
    "print(\"A few items at the end of the vocabulary list\")\n",
    "print(voc_l[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary dictionary, key is the word, value is a unique integer\n",
      ":0\n",
      "<:1\n",
      ">:2\n",
      "Agra:3\n",
      "Ahmedabad:4\n",
      "Amritsar:5\n",
      "Bengaluru:6\n",
      "Chennai:7\n",
      "Delhi:8\n",
      "Goa:9\n",
      "Hyderabad:10\n",
      "Jaipur:11\n",
      "Kochi:12\n",
      "Kolkata:13\n",
      "Mumbai:14\n",
      "Rishikesh:15\n",
      "Udaipur:16\n",
      "Varanasi:17\n"
     ]
    }
   ],
   "source": [
    "# vocab: dictionary that has the index of the corresponding words\n",
    "vocab = {} \n",
    "\n",
    "# Get the index of the corresponding words. \n",
    "for i, word in enumerate(sorted(voc_l)): \n",
    "    vocab[word] = i       \n",
    "    \n",
    "print(\"Vocabulary dictionary, key is the word, value is a unique integer\")\n",
    "cnt = 0\n",
    "for k,v in vocab.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '<': 1,\n",
       " '>': 2,\n",
       " 'Agra': 3,\n",
       " 'Ahmedabad': 4,\n",
       " 'Amritsar': 5,\n",
       " 'Bengaluru': 6,\n",
       " 'Chennai': 7,\n",
       " 'Delhi': 8,\n",
       " 'Goa': 9,\n",
       " 'Hyderabad': 10,\n",
       " 'Jaipur': 11,\n",
       " 'Kochi': 12,\n",
       " 'Kolkata': 13,\n",
       " 'Mumbai': 14,\n",
       " 'Rishikesh': 15,\n",
       " 'Udaipur': 16,\n",
       " 'Varanasi': 17}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = ['Goa', 'Rishikesh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dictionaries(training_corpus, vocab):\n",
    "\n",
    "    \n",
    "\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "\n",
    "    prev_tag = '--s--' \n",
    "    \n",
    "\n",
    "    i = 0 \n",
    "    \n",
    "\n",
    "    for word_tag in training_corpus:\n",
    "        \n",
    "\n",
    "        i += 1\n",
    "        \n",
    "      \n",
    "        if i % 50000 == 0:\n",
    "            print(f\"word count = {i}\")\n",
    " \n",
    "        word, tag = get_word_tag(word_tag, vocab)\n",
    "        \n",
    "     \n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        \n",
    "\n",
    "        emission_counts[(tag, word)] += 1\n",
    "\n",
    "        # Increment the tag count\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "      \n",
    "        prev_tag = tag\n",
    "        \n",
    "\n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POS tags (number of 'states'): 32\n",
      "View these POS tags (states)\n",
      "['hidden_0', 'hidden_1', 'hidden_10', 'hidden_11', 'hidden_12', 'hidden_13', 'hidden_14', 'hidden_15', 'hidden_16', 'hidden_17', 'hidden_18', 'hidden_19', 'hidden_2', 'hidden_20', 'hidden_21', 'hidden_22', 'hidden_23', 'hidden_24', 'hidden_25', 'hidden_26', 'hidden_27', 'hidden_28', 'hidden_29', 'hidden_3', 'hidden_30', 'hidden_31', 'hidden_4', 'hidden_5', 'hidden_6', 'hidden_7', 'hidden_8', 'hidden_9']\n"
     ]
    }
   ],
   "source": [
    "# get all the POS states\n",
    "states = sorted(tag_counts.keys())\n",
    "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
    "print(\"View these POS tags (states)\")\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# Part 2: Hidden Markov Models for POS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "## Part 2.1 Generating Matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: create_transition_matrix\n",
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    # Count the number of unique POS tags\n",
    "    num_tags = len(all_tags)\n",
    "    \n",
    "    # Initialize the transition matrix 'A'\n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    \n",
    "    # Get the unique transition tuples (previous POS, current POS)\n",
    "    trans_keys = set(transition_counts.keys())\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ### \n",
    "    \n",
    "    # Go through each row of the transition matrix A\n",
    "    for i in range(num_tags):\n",
    "        \n",
    "        # Go through each column of the transition matrix A\n",
    "        for j in range(num_tags):\n",
    "\n",
    "            # Initialize the count of the (prev POS, current POS) to zero\n",
    "            count = 0\n",
    "        \n",
    "            # Define the tuple (prev POS, current POS)\n",
    "            # Get the tag at position i and tag at position j (from the all_tags list)\n",
    "            key = (all_tags[i], all_tags[j])\n",
    "\n",
    "            # Check if the (prev POS, current POS) tuple \n",
    "            # exists in the transition counts dictionary\n",
    "            if key in transition_counts: #complete this line\n",
    "                \n",
    "                # Get count from the transition_counts dictionary \n",
    "                # for the (prev POS, current POS) tuple\n",
    "                count = transition_counts[key]\n",
    "                \n",
    "            # Get the count of the previous tag (index position i) from tag_counts\n",
    "            count_prev_tag = tag_counts[all_tags[i]]\n",
    "            \n",
    "            # Apply smoothing using count of the tuple, alpha, \n",
    "            # count of previous tag, alpha, and total number of tags\n",
    "            A[i,j] = (count + alpha) / (count_prev_tag + alpha * num_tags)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A at row 0, col 0: 0.352840285\n",
      "A at row 3, col 1: 0.0833\n",
      "View a subset of transition matrix A\n",
      "          hidden_8  hidden_9\n",
      "hidden_8  0.000077  0.000077\n",
      "hidden_9  0.380420  0.000048\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "# Testing your function\n",
    "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
    "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
    "\n",
    "print(\"View a subset of transition matrix A\")\n",
    "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
    "print(A_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the 'B' emission probabilities matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: create_emission_matrix\n",
    "\n",
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "\n",
    "    \n",
    "    # get the number of POS tag\n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # Get a list of all POS tags\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    " \n",
    "    num_words = len(vocab)\n",
    "    \n",
    "\n",
    "    B = np.zeros((num_tags, num_words))\n",
    "    \n",
    "\n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "\n",
    "    for i in range(num_tags): # complete this line\n",
    "        \n",
    "\n",
    "        for j in range(num_words): # complete this line\n",
    "\n",
    "\n",
    "            count = 0\n",
    "       \n",
    "            key =  (all_tags[i], vocab[j])\n",
    "\n",
    "\n",
    "            if key in emission_counts: # complete this line\n",
    "        \n",
    "      \n",
    "                count = emission_counts[key]\n",
    "                \n",
    "\n",
    "            count_tag = tag_counts[all_tags[i]]\n",
    "                \n",
    "\n",
    "            B[i,j] = (count + alpha) / (count_tag + alpha * num_words)\n",
    "\n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating your emission probability matrix. this takes a few minutes to run. \n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "# Part 3: Viterbi Algorithm and Dynamic Programming\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent infinity and negative infinity like this:\n",
    "\n",
    "```CPP\n",
    "float('inf')\n",
    "float('-inf')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    \n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    # Initialize best_probs matrix \n",
    "\n",
    "    best_probs = np.zeros((num_tags, len(corpus)))\n",
    "    \n",
    "    # Initialize best_paths matrix\n",
    " \n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
    "    \n",
    "    # Define the start token\n",
    "    s_idx = states.index(\"hidden_0\")\n",
    " \n",
    "    \n",
    "\n",
    "    for i in range(num_tags): # complete this line\n",
    "        \n",
    "        \n",
    "        if A[s_idx,i] == 0: # complete this line\n",
    "            \n",
    "            # Initialize best_probs at POS tag 'i', column 0, to negative infinity\n",
    "            best_probs[i,0] = -float('inf')\n",
    "        \n",
    "   \n",
    "        else:\n",
    "            \n",
    "\n",
    "            best_probs[i,0] = np.log(A[s_idx, i]) + np.log(B[i, vocab[corpus[0]]] )\n",
    "\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,0]: -12.5746\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test the function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_probs[0,0]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_probs[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_paths[2,3]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_paths[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \n",
    "print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "## Part 3.2 Viterbi Forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: viterbi_forward\n",
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
    "\n",
    "    # Get the number of unique POS tags (which is the num of rows in best_probs)\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "\n",
    "    for i in range(1, len(test_corpus)): \n",
    "\n",
    "\n",
    "        for j in range(num_tags): # complete this line\n",
    "\n",
    "            best_prob_i = -float('inf')\n",
    "            \n",
    "\n",
    "            best_path_i = None\n",
    "\n",
    "       \n",
    "            for k in range(num_tags): # complete this line\n",
    "            \n",
    "\n",
    "                prob = best_probs[k, i - 1] + math.log(A[k, j]) + math.log(B[j, vocab[test_corpus[i]]])\n",
    "\n",
    "                # check if this path's probability is greater than\n",
    "                # the best probability up to and before this point\n",
    "                if prob > best_prob_i: # complete this line\n",
    "                    \n",
    "                    # Keep track of the best probability\n",
    "                    best_prob_i = prob\n",
    "                    \n",
    "         \n",
    "                    best_path_i = k\n",
    "\n",
    "\n",
    "            best_probs[j,i] = best_prob_i\n",
    "            \n",
    "     \n",
    "            best_paths[j,i] = best_path_i\n",
    "\n",
    "\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,1]: -17.2580\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test this function \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_probs[0,1]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_probs[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_probs[0,4]: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_probs[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "# Test this function \n",
    "print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \n",
    "print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected Output\n",
    "\n",
    "```CPP\n",
    "best_probs[0,1]: -24.7822\n",
    "best_probs[0,4]: -49.5601\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "## Part 3.3 Viterbi backward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: viterbi_backward\n",
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    '''\n",
    "    This function returns the best path.\n",
    "    \n",
    "    '''\n",
    "    # Get the number of words in the corpus\n",
    "    # which is also the number of columns in best_probs, best_paths\n",
    "    m = best_paths.shape[1] \n",
    "    \n",
    "    # Initialize array z, same length as the corpus\n",
    "    z = [None] * m\n",
    "    \n",
    "    # Get the number of unique POS tags\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Initialize the best probability for the last word\n",
    "    best_prob_for_last_word = -float('inf')\n",
    "    \n",
    "    # Initialize pred array, same length as corpus\n",
    "    pred = [None] * m\n",
    "\n",
    "    for k in range(num_tags):\n",
    "\n",
    "\n",
    "        if best_probs[k, m - 1] > best_prob_for_last_word:\n",
    "            \n",
    "            # Store the new best probability for the lsat word\n",
    "            best_prob_for_last_word = best_probs[k, m - 1]\n",
    "    \n",
    " \n",
    "            z[m - 1] = k\n",
    "            \n",
    "\n",
    "    pred[m - 1] = states[z[m - 1]]\n",
    "\n",
    "    for i in range(m - 1, -1, -1): # complete this line\n",
    "        \n",
    "\n",
    "        pos_tag_for_word_i = z[i]\n",
    "        \n",
    "\n",
    "        z[i - 1] = best_paths[pos_tag_for_word_i, i]\n",
    "        \n",
    " \n",
    "        pred[i - 1] = states[z[i - 1]]\n",
    "        \n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for pred[-7:m-1] is: \n",
      " ['Goa'] \n",
      " ['hidden_6'] \n",
      "\n",
      "The prediction for pred[0:8] is: \n",
      " ['hidden_6', 'hidden_0'] \n",
      " ['Goa', 'Rishikesh']\n"
     ]
    }
   ],
   "source": [
    "# Run and test your function\n",
    "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
    "m=len(pred)\n",
    "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
    "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read hidden encoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hidden_encoders.txt\", 'r') as f:\n",
    "    hidden_states = f.readlines()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"routes.txt\", 'r') as f:\n",
    "    routes = f.readlines()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden_6', 'hidden_0']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = []\n",
    "hidden_stat = {}\n",
    "for x in hidden_states:\n",
    "    f.append(x.split('\\t'))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_0': [0, 1, 2, 3, 4, 5, 6, 10, 13, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33, 38, 39, 40, 41, 42, 43, 44, 55, 56, 57, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 82, 86, 87, 89, 91, 99, 10], 'hidden_1': [0, 1, 2, 3, 13, 21, 22, 23, 24, 26, 27, 28, 31, 32, 33, 41, 42, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 99, 10], 'hidden_2': [0, 1, 6, 10, 13, 17, 27, 28, 31, 32, 33, 38, 39, 40, 43, 44, 69, 70, 71, 72, 73, 74, 75, 82, 9], 'hidden_3': [0, 1, 6, 18, 31, 32, 33, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 86, 87, 8], 'hidden_4': [0, 1, 10, 13, 25, 26, 31, 32, 33, 66, 67, 68, 69, 70, 71, 72, 73, 89, 9], 'hidden_5': [1, 2, 3, 4, 5, 6, 13, 21, 22, 23, 24, 25, 26, 27, 28, 64, 65, 73, 7], 'hidden_6': [1, 2, 3, 4, 10, 13, 17, 21, 22, 25, 26, 27, 28, 38, 39, 40, 42, 43, 55, 56, 57, 73, 74, 9], 'hidden_7': [4, 10, 25, 26, 27, 28, 66, 67, 68, 87, 9], 'hidden_8': [4, 13, 26, 27, 28, 38, 39, 40, 41, 73, 74, 7], 'hidden_9': [4, 25, 26, 33, 38, 39, 40, 41, 42, 43, 44, 55, 56, 57, 73, 74, 75, 8], 'hidden_10': [5, 6, 25, 26, 27, 28, 55, 56, 57, 64, 65, 66, 67, 68, 73, 74, 86, 8], 'hidden_11': [5, 10, 13, 17, 18, 33, 41, 42, 44, 65, 66, 67, 68, 69, 70, 71, 82, 86, 87, 9], 'hidden_12': [7, 8, 9, 11, 12, 14, 15, 16, 19, 20, 29, 30, 34, 35, 36, 37, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 58, 59, 60, 61, 62, 63, 76, 77, 78, 79, 80, 81, 83, 84, 85, 88, 90, 92, 93, 94, 95, 96, 97, 98, 100, 10], 'hidden_13': [7, 8, 9, 11, 12, 14, 15, 16, 29, 30, 61, 62, 76, 77, 78, 79, 80, 81, 83, 84, 8], 'hidden_14': [7, 8, 9, 11, 15, 16, 45, 47, 48, 49, 50, 59, 63, 76, 83, 84, 93, 94, 96, 100, 10], 'hidden_15': [7, 11, 20, 29, 30, 53, 54, 63, 76, 77, 78, 80, 81, 96, 97, 10], 'hidden_16': [9, 11, 29, 30, 36, 47, 48, 52, 76, 77, 78, 79, 80, 81, 93, 94, 95, 96, 9], 'hidden_17': [9, 11, 30, 34, 35, 47, 48, 59, 85, 93, 94, 95, 9], 'hidden_18': [9, 14, 15, 16, 20, 29, 30, 34, 35, 45, 47, 48, 49, 50, 51, 58, 60, 81, 83, 84, 85, 95, 96, 10], 'hidden_19': [9, 20, 34, 35, 47, 48, 63, 76, 81, 85, 88, 90, 92, 9], 'hidden_20': [10, 18, 21, 22, 23, 24, 26, 27, 28, 33, 41, 42, 64, 65, 66, 67, 68, 69, 70, 71, 74, 75, 82, 89, 91, 10], 'hidden_21': [11, 12, 34, 35, 37, 48, 49, 50, 59, 76, 83, 84, 93, 9], 'hidden_22': [11, 14, 15, 16, 30, 53, 54, 8], 'hidden_23': [11, 19, 20, 29, 46, 47, 48, 49, 50, 51, 52, 54, 58, 59, 60, 61, 62, 76, 77, 78, 79, 80, 81, 94, 9], 'hidden_24': [11, 19, 46, 47, 48, 49, 50, 53, 54, 76, 77, 78, 79, 80, 81, 83, 84, 96, 9], 'hidden_25': [11, 20, 36, 37, 53, 54, 76, 77, 80, 81, 8], 'hidden_26': [12, 29, 30, 76, 10], 'hidden_27': [20, 47, 48, 49, 50, 51, 85, 92, 94, 95, 9], 'hidden_28': [20, 59, 76, 77, 78, 85, 88, 9], 'hidden_29': [21, 22, 26, 27, 28, 44, 57, 86, 10], 'hidden_30': [21, 25, 26, 33, 38, 39, 40, 41, 55, 56, 57, 73, 74, 7], 'hidden_31': [38, 39, 40, 41, 42, 43, 44, 73, 74, 7]}\n"
     ]
    }
   ],
   "source": [
    "hidden_stat = {}\n",
    "\n",
    "for sublist in f:\n",
    "    key = sublist[0]  # Extract the key\n",
    "    value_str = sublist[1].strip()[1:-2]  # Extract the string of numbers and remove unnecessary characters\n",
    "    value_list = [int(num) for num in value_str.split(', ')]  # Convert the string of numbers to a list of integers\n",
    "    hidden_stat[key] = value_list  # Assign the value list to the dictionary key\n",
    "\n",
    "print(hidden_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden_6', 'hidden_0']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find the intersection of routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_routes = set(hidden_stat[pred[0]]).intersection(hidden_stat[pred[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'Rishikesh', 'Hyderabad', 'Chennai', 'Kochi', 'Kolkata', 'Goa', 'Rishikesh', '>']\n",
      "\n",
      "['<', 'Kolkata', 'Goa', 'Rishikesh', 'Kolkata', '>']\n",
      "\n",
      "['<', 'Goa', 'Rishikesh', 'Kolkata', 'Goa', '>']\n",
      "\n",
      "['<', 'Kolkata', 'Goa', 'Delhi', 'Udaipur', 'Amritsar', 'Kolkata', '>']\n",
      "\n",
      "['<', 'Delhi', 'Bengaluru', 'Mumbai', 'Kochi', 'Hyderabad', 'Goa', 'Delhi', '>']\n",
      "\n",
      "['<', 'Kochi', 'Rishikesh', 'Amritsar', 'Kolkata', 'Goa', 'Hyderabad', 'Mumbai', 'Kochi', '>']\n",
      "\n",
      "['<', 'Mumbai', 'Kochi', 'Goa', 'Mumbai', '>']\n",
      "\n",
      "['<', 'Kolkata', 'Ahmedabad', 'Bengaluru', 'Agra', 'Goa', 'Rishikesh', 'Kolkata', '>']\n",
      "\n",
      "['<', 'Bengaluru', 'Agra', 'Goa', 'Rishikesh', 'Kolkata', 'Bengaluru', '>']\n",
      "\n",
      "['<', 'Kolkata', 'Goa', 'Udaipur', 'Hyderabad', 'Ahmedabad', 'Varanasi', 'Delhi', 'Kolkata', '>']\n",
      "\n",
      "['<', 'Goa', 'Udaipur', 'Hyderabad', 'Ahmedabad', 'Varanasi', 'Delhi', 'Kolkata', 'Bengaluru', 'Agra', 'Amritsar', 'Rishikesh', 'Goa', '>']\n",
      "\n",
      "['<', 'Varanasi', 'Delhi', 'Kolkata', 'Bengaluru', 'Agra', 'Amritsar', 'Rishikesh', 'Goa', 'Kochi', 'Varanasi', '>']\n",
      "\n",
      "['<', 'Delhi', 'Kolkata', 'Bengaluru', 'Agra', 'Amritsar', 'Rishikesh', 'Goa', 'Kochi', 'Varanasi', 'Delhi', '>']\n",
      "\n",
      "['<', 'Amritsar', 'Ahmedabad', 'Jaipur', 'Kochi', 'Goa', 'Udaipur', 'Amritsar', '>']\n",
      "\n",
      "['<', 'Ahmedabad', 'Jaipur', 'Kochi', 'Goa', 'Udaipur', 'Amritsar', 'Ahmedabad', '>']\n",
      "\n",
      "['<', 'Jaipur', 'Kochi', 'Goa', 'Udaipur', 'Amritsar', 'Ahmedabad', 'Jaipur', '>']\n",
      "\n",
      "['<', 'Jaipur', 'Bengaluru', 'Mumbai', 'Rishikesh', 'Udaipur', 'Goa', 'Jaipur', '>']\n",
      "\n",
      "['<', 'Udaipur', 'Goa', 'Jaipur', 'Kochi', 'Udaipur', '>']\n",
      "\n",
      "['<', 'Varanasi', 'Ahmedabad', 'Goa', 'Udaipur', 'Varanasi', '>']\n",
      "\n",
      "['<', 'Ahmedabad', 'Goa', 'Udaipur', 'Varanasi', 'Ahmedabad', '>']\n",
      "\n",
      "['<', 'Goa', 'Udaipur', 'Varanasi', 'Ahmedabad', 'Agra', 'Goa', '>']\n",
      "\n",
      "['<', 'Rishikesh', 'Hyderabad', 'Chennai', 'Kochi', 'Varanasi', 'Kolkata', 'Goa', 'Ahmedabad', 'Udaipur', 'Amritsar', 'Jaipur', 'Rishikesh', '>']\n",
      "\n",
      "['<', 'Kochi', 'Varanasi', 'Kolkata', 'Goa', 'Ahmedabad', 'Udaipur', 'Amritsar', 'Jaipur', 'Rishikesh', 'Bengaluru', 'Kochi', '>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in recommended_routes:\n",
    "    print(routes[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "# Part 4: Predicting on a data set\n",
    "\n",
    "Compute the accuracy of your prediction by comparing it with the true `y` labels. \n",
    "- `pred` is a list of predicted POS tags corresponding to the words of the `test_corpus`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe third word is:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mprep\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYour prediction is:\u001b[39m\u001b[38;5;124m'\u001b[39m, pred[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYour corresponding label y is: \u001b[39m\u001b[38;5;124m'\u001b[39m, y[\u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('The third word is:', prep[3])\n",
    "print('Your prediction is:', pred[3])\n",
    "print('Your corresponding label y is: ', y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC2-2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
